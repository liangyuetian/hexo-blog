---
title: boa 引擎学习：第一部分，词法解析
tags:
  - rust
  - boa
  - aka
  - 词法解析
categories:
  - Rust
  - Boa
abbrlink: 3441890087
date: 2023-03-09 23:15:53
---

这一篇是认真学习 boa 引擎的一个开篇，记录了一周之后看源码的结果，以及对各种名词的概念和理解

<!-- more -->

# 1. 词法解析需要的数据结构

如何解析 <code>let a1 = 'x2';</code> 这句简短的语句

* 表达式部分：'x1'
* 语句部分：let a1 = 'x2';


**表达式（Expression）有值，而语句（Statement）不总有。** 膜拜大佬的简言意骇

## 1.1 将代码字符串 解析为 **代码单元**

在 UTF-16（Unicode） 中，绝大部分常用的字符可以用一个 16 位的值表示（即一个代码单元）。

相关资料:
* [UTF-16](https://zh.wikipedia.org/zh-cn/UTF-16)
* [码元](https://developer.mozilla.org/zh-CN/docs/Glossary/Code_unit)
* [String.fromCharCode()](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/String/fromCharCode)

创建 <code>Source</code> 结构体

```rust
#[derive(Debug)]
pub struct Source<'path, R> {
    pub(crate) reader: R,
    pub(crate) path: Option<&'path Path>,
}
```

包含 2个部分：
* reader: 源码的 utf-16 值
* path: 源码的路径，Option 的意思是可以为 None

结果如下：
![source 的内部](img_1.png)

## 1.2 解析给定的代码脚本

### 1.2.1 创建 <code>Parser</code> 解析器

```rust
let mut parser = Parser::new(src);
```

```rust
/// Parser for the ECMAScript language.
///
/// This parser implementation tries to be conformant to the most recent
/// [ECMAScript language specification], and it also implements some legacy features like
/// [labelled functions][label] or [duplicated block-level function definitions][block].
///
/// [spec]: https://tc39.es/ecma262/#sec-ecmascript-language-source-code
/// [label]: https://tc39.es/ecma262/#sec-labelled-function-declarations
/// [block]: https://tc39.es/ecma262/#sec-block-duplicates-allowed-static-semantics
#[derive(Debug)]
#[allow(unused)] // Right now the path is not used, but it's better to have it for future improvements.
pub struct Parser<'a, R> {
    /// Path to the source being parsed.
    path: Option<&'a Path>,
    /// Cursor of the parser, pointing to the lexer and used to get tokens for the parser.
    cursor: Cursor<R>,
}
```

```rust
/// Token cursor.
///
/// This internal structure gives basic testable operations to the parser.
#[derive(Debug)]
pub(super) struct Cursor<R> {
    buffered_lexer: BufferedLexer<R>,

    /// Tracks the number of nested private environments that the cursor is in.
    private_environment_nested_index: usize,

    /// Tracks the number of private environments on the root level of the code that is parsed.
    private_environment_root_index: usize,

    /// Tracks if the cursor is in a arrow function declaration.
    arrow: bool,

    /// Indicate if the cursor is used in `JSON.parse`.
    json_parse: bool,

    /// Indicate if the cursor's **goal symbol** is a Module.
    module: bool,
}


impl<R> Cursor<R>
    where
        R: Read,
{
    /// Creates a new cursor with the given reader.
    pub(super) fn new(reader: R) -> Self {
        Self {
            buffered_lexer: Lexer::new(reader).into(),
            private_environment_nested_index: 0,
            private_environment_root_index: 0,
            arrow: false,
            json_parse: false,
            module: false,
        }
    }
}

```

解析器包含两个部分，源码的 **路径**和一个 解析器的指针的**光标**，用于描述解析器解析到哪里，Cursor::new 传入 源码的 utf-16 值

cursor 中创建了 词法分析器的实例 **Lexer*

```rust
/// Lexer or tokenizer for the Boa JavaScript Engine.
#[derive(Debug)]
pub struct Lexer<R> {
    cursor: Cursor<R>,
    goal_symbol: InputElement,
}

#[derive(Debug)]
pub(super) struct BufferedLexer<R> {
    lexer: Lexer<R>,
    peeked: [Option<Token>; PEEK_BUF_SIZE],
    read_index: usize,
    write_index: usize,
}

/// 指针中包含了 源码的迭代器，分析的位置，是否是严格模式
#[derive(Debug)]
pub(super) struct Cursor<R> {
    iter: InnerIter<R>, // 用于循环 utf-16 值
    pos: Position, // 包含第几行第几列
    strict_mode: bool,
}
```
parser 执行结果
![解析器初始化完成后的结构](parser.png)


# 2. 接下来就是将输入的代码解析为 **boa AST**
## 2.1 解析入口

脚本的处理一共有2个类，**Script** 和 **ScriptBody**
```rust

/// Parses a full script.
///
/// More information:
///  - [ECMAScript specification][spec]
///
/// [spec]: https://tc39.es/ecma262/#prod-Script
#[derive(Debug, Clone, Copy)]
pub struct Script {
    direct_eval: bool,
}

/// Parses a script body.
///
/// More information:
///  - [ECMAScript specification][spec]
///
/// [spec]: https://tc39.es/ecma262/#prod-ScriptBody
#[derive(Debug, Clone, Copy)]
pub struct ScriptBody {
    directive_prologues: bool,
    strict: bool, // 是否是严格模式
    direct_eval: bool,
}
```
在 ScriptBody 创建了 statementList，

```rust
let statement_list = ScriptBody::new(true, cursor.strict_mode(), self.direct_eval)
            .parse(cursor, interner)?;
```

在 ScriptBody 的 price 方法内部，创建了 parser::statement::StatementList，并传入了 cursor，在 cursor 中保存着词法分析器和源码本身

```rust
let body = self::statement::StatementList::new(
            false,
            false,
            false,
            &[],
            self.directive_prologues,
            self.strict,
        )
        .parse(cursor, interner)?;
```

在 parse 内部 词法分析器开始第一次执行，使用 peek 方法，查看当前位置的下一个字节是什么， 一直循环

```rust
 loop {
    match cursor.peek(0, interner) ? {
        Some(token) if self.break_nodes.contains(token.kind()) => break,
        Some(token) if directive_prologues & & string_literal_escape_sequence.is_none() => {
        if let TokenKind::StringLiteral((_, Some(escape_sequence))) = token.kind() {
            string_literal_escape_sequence =
                 Some((token.span().start(), * escape_sequence));
            }
        }
        None => break,
        _ => {}
    }
}
```
接下来就是 lexer 进行大展神威的时候了，

peek 创建一个 loop 循环，直到找一个第一个不可分割的 token，这里我们的源码是：<code>let a1 = 'x2';</code>，这里loop出来的结果应该是 let
```rust
let mut read_index = self.read_index; // nadao 
let mut count = 0;
let res_token = loop {
    if read_index == self.write_index {
        self.fill(interner)?; // 当读取的位置和写入的位置一样时，将下一个读取到的token存到缓冲区（peeked）中
    }
    
    if let Some(ref token) = self.peeked[read_index] {
        if skip_line_terminators && token.kind() == &TokenKind::LineTerminator {
            read_index = (read_index + 1) % PEEK_BUF_SIZE;
            // We only store 1 contiguous line terminator, so if the one at `self.read_index`
            // was a line terminator, we know that the next won't be one.
            if read_index == self.write_index {
                self.fill(interner)?;
            }
        }
        if count == skip_n {
            break self.peeked[read_index].as_ref();
        }
    } else {
        break None;
    }
    read_index = (read_index + 1) % PEEK_BUF_SIZE;
    count += 1;
};
```

当读取的位置和写入的位置一样时，将下一个读取到的token存到缓冲区（peeked）中
fill 首先读取 下一个缓冲区的 token，如果没有就调用 lexer 词法分析器 next方法 读取token

lexer next 方法到底是怎么取出的一个最小的代码单元token呢？这里的代码依旧是：<code>let a1 = 'x2';</code>

1. 调用 lexer.custor.iter.next_char() 取出第一个字符的 utf-16 值：108，执行 String.fromCharCode(108) 将108转成字母，就是 “l”
2. 如果接下来的字符不是 0xD | 0xA | 0x2028 | 0x2029；即 换行符 \n\r，就调用 **next_column** 将指针移动到下一个字符，更新 pos 字段，移到下一个字符的位置，l 的pos是(1,1),此时变成 (1,2)
3. 下面继续判断 l 到底是什么，以及该执行什么操作

## 2.2 解析声明语句

```rust=
fn next() {
    if let Ok(c) = char::try_from(next_ch) {
        let token = match c {
            '\r' | '\n' | '\u{2028}' | '\u{2029}' => Ok(Token::new(
                TokenKind::LineTerminator,
                Span::new(start, self.cursor.pos()),
            )),
            '"' | '\'' => StringLiteral::new(c).lex(&mut self.cursor, start, interner),
            '`' => TemplateLiteral.lex(&mut self.cursor, start, interner),
            ';' => Ok(Token::new(
                Punctuator::Semicolon.into(),
                Span::new(start, self.cursor.pos()),
            )),
            ':' => Ok(Token::new(
                Punctuator::Colon.into(),
                Span::new(start, self.cursor.pos()),
            )),
            '.' => {
                if self.cursor.peek()?.as_ref().map(u8::is_ascii_digit) == Some(true) {
                    NumberLiteral::new(b'.').lex(&mut self.cursor, start, interner)
                } else {
                    SpreadLiteral::new().lex(&mut self.cursor, start, interner)
                }
            }
            '(' => Ok(Token::new(
                Punctuator::OpenParen.into(),
                Span::new(start, self.cursor.pos()),
            )),
            ')' => Ok(Token::new(
                Punctuator::CloseParen.into(),
                Span::new(start, self.cursor.pos()),
            )),
            ',' => Ok(Token::new(
                Punctuator::Comma.into(),
                Span::new(start, self.cursor.pos()),
            )),
            '{' => Ok(Token::new(
                Punctuator::OpenBlock.into(),
                Span::new(start, self.cursor.pos()),
            )),
            '}' => Ok(Token::new(
                Punctuator::CloseBlock.into(),
                Span::new(start, self.cursor.pos()),
            )),
            '[' => Ok(Token::new(
                Punctuator::OpenBracket.into(),
                Span::new(start, self.cursor.pos()),
            )),
            ']' => Ok(Token::new(
                Punctuator::CloseBracket.into(),
                Span::new(start, self.cursor.pos()),
            )),
            '#' => PrivateIdentifier::new().lex(&mut self.cursor, start, interner),
            '/' => self.lex_slash_token(start, interner),
            #[allow(clippy::cast_possible_truncation)]
            '=' | '*' | '+' | '-' | '%' | '|' | '&' | '^' | '<' | '>' | '!' | '~' | '?' => {
                Operator::new(next_ch as u8).lex(&mut self.cursor, start, interner)
            }
            '\\' if self.cursor.peek()? == Some(b'u') => {
                Identifier::new(c).lex(&mut self.cursor, start, interner)
            }
            _ if Identifier::is_identifier_start(c as u32) => {
                Identifier::new(c).lex(&mut self.cursor, start, interner) // TODO 执行到了这里
            }
            #[allow(clippy::cast_possible_truncation)]
            _ if c.is_ascii_digit() => {
                NumberLiteral::new(next_ch as u8).lex(&mut self.cursor, start, interner)
            }
            _ => {
                let details = format!(
                    "unexpected '{c}' at line {}, column {}",
                    start.line_number(),
                    start.column_number()
                );
                Err(Error::syntax(details, start))
            }
        }?;

        if token.kind() == &TokenKind::Comment {
            // Skip comment
            self.next(interner)
        } else {
            Ok(Some(token))
        }
    } else {
        Err(Error::syntax(
            format!(
                "unexpected utf-8 char '\\u{next_ch}' at line {}, column {}",
                start.line_number(),
                start.column_number()
            ),
            start,
        ))
    }
}
```
在这里就和其他的语言构建ast一样的判断了，这里执行到 62 行，创建了一个标识符
```rust
impl<R> Tokenizer<R> for Identifier {
    fn lex(
        &mut self,
        cursor: &mut Cursor<R>,
        start_pos: Position,
        interner: &mut Interner,
    ) -> Result<Token, Error>
    where
        R: Read,
    {
        let _timer = Profiler::global().start_event("Identifier", "Lexing");

        let (identifier_name, contains_escaped_chars) =
            Self::take_identifier_name(cursor, start_pos, self.init)?;

        let token_kind = match identifier_name.parse() {
            Ok(Keyword::True) => TokenKind::BooleanLiteral(true),
            Ok(Keyword::False) => TokenKind::BooleanLiteral(false),
            Ok(Keyword::Null) => TokenKind::NullLiteral,
            Ok(keyword) => TokenKind::Keyword((keyword, contains_escaped_chars)),
            _ => TokenKind::IdentifierName((
                interner.get_or_intern(identifier_name.as_str()),
                ContainsEscapeSequence(contains_escaped_chars),
            )),
        };

        Ok(Token::new(token_kind, Span::new(start_pos, cursor.pos())))
    }
}
```

在这里调用 take_identifier_name 方法一直 loop next_char 来获取下一个字符，并使用 <code>Self::is_identifier_part(ch)</code> 方法来判断字符的合法性
如果字符 合法就将下一个字符添加到 identifier_name 字段中，到此为止就将 let 都解析出来了，保存到 identifier_name 中

identifier_name.parse() 会调用 string.from_str 方法， 会对 let 字符串返回 Keyword:Let

```rust
impl FromStr for Keyword {
    type Err = KeywordError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "await" => Ok(Self::Await),
            "async" => Ok(Self::Async),
            "break" => Ok(Self::Break),
            "case" => Ok(Self::Case),
            "catch" => Ok(Self::Catch),
            "class" => Ok(Self::Class),
            "continue" => Ok(Self::Continue),
            "const" => Ok(Self::Const),
            "debugger" => Ok(Self::Debugger),
            "default" => Ok(Self::Default),
            "delete" => Ok(Self::Delete),
            "do" => Ok(Self::Do),
            "else" => Ok(Self::Else),
            "enum" => Ok(Self::Enum),
            "extends" => Ok(Self::Extends),
            "export" => Ok(Self::Export),
            "false" => Ok(Self::False),
            "finally" => Ok(Self::Finally),
            "for" => Ok(Self::For),
            "function" => Ok(Self::Function),
            "if" => Ok(Self::If),
            "in" => Ok(Self::In),
            "instanceof" => Ok(Self::InstanceOf),
            "import" => Ok(Self::Import),
            "let" => Ok(Self::Let),
            "new" => Ok(Self::New),
            "null" => Ok(Self::Null),
            "of" => Ok(Self::Of),
            "return" => Ok(Self::Return),
            "super" => Ok(Self::Super),
            "switch" => Ok(Self::Switch),
            "this" => Ok(Self::This),
            "throw" => Ok(Self::Throw),
            "true" => Ok(Self::True),
            "try" => Ok(Self::Try),
            "typeof" => Ok(Self::TypeOf),
            "var" => Ok(Self::Var),
            "void" => Ok(Self::Void),
            "while" => Ok(Self::While),
            "with" => Ok(Self::With),
            "yield" => Ok(Self::Yield),
            _ => Err(KeywordError),
        }
    }
}
```

生成：Token::new(token_kind, Span::new(start_pos, cursor.pos()))

到目前为止生成了 let 的 identifier token，此时 read_index + 1, write_index + 1
![Leyword::Let.png](let.png)

## 2.3 解析变量名

parser::statement.parse 中的 loop 继续执行，

在 StatementListItem 继续 parser 的时候，

cursor 继续向后解析，于是解析出另一个字符串 a1，进行关键词匹配，发现匹配不了任意一个关键字，
   于是创建：IdentifierName，并判断是 utf8 还是 utf16
   在解析到 a1 这样的字符串时，会调用 boa-interner 这个包来处理字符串，提升编译器性能，对字符串进行单独处理，提高速度，
   同时返回了utf8和utf16，保存到 Interner 中，即boa引擎启动时创建上下问中的**interner**（字符串本身在编程语言中时一个比较复杂的东西，但是es从语言层面上隐去了字符串的复杂性，于是在只能引擎自己去使用自己的方式优化js字符串的性能，于是js 字符串写起来简单又舒服，都在引擎在负重前行呀）
   字符串类型为：JStrRef
   字符串保存在 Cow 中，一种智能指针中
![字符串优化](字符串优化.png)


## 2.4 解析 符号，即 = 赋值语句
1. 在 变量名解析处理出来之后，又紧接着匹配标点符号，于是找到了 = 号
   **'=' | '*' | '+' | '-' | '%' | '|' | '&' | '^' | '<' | '>' | '!' | '~' | '?' => { Operator::new(next_ch as u8).lex(&mut self.cursor, start, interner) }**
   生成了 Operator 操作符类型的 ast

![等号匹配](等号匹配.png)
![=号操作符](punctuator.assign=.png)

## 2.5 解析值
后面继续匹配

1. 如果是=号，那后面的解析交给 **AssignmentExpression** 赋值表达式解析，虽然比较耦合，但是各司其职
   cursor 指针继续往下走，当匹配到 " | ' 时，则会，创建 StringLiteral 类型的token，顾名思义，是用来描述字符串的
   <pre><code>'"' | '\'' => StringLiteral::new(c).lex(&mut self.cursor, start, interner),</code></pre>
2. 取出已经解析之后的 kind 匹不匹配 function,async,class,let,const，如果发现是Let，
   就创建 LexicalDeclaration，接下来由 LexicalDeclaration::parse 来接管 cursor，
3. 在赋值语句解析完毕之后会创建一个 **Variable::BindingIdentifier** 变量的声明
4. 在 函数的最后，会判断是否是 const，创建块级作用域

```rust
/// Variable represents a variable declaration of some kind.
///
/// For `let` and `const` declarations this type represents a [`LexicalBinding`][spec1]
///
/// For `var` declarations this type represents a [`VariableDeclaration`][spec2]
///
/// More information:
///  - [ECMAScript reference: 14.3 Declarations and the Variable Statement][spec3]
///
/// [spec1]: https://tc39.es/ecma262/#prod-LexicalBinding
/// [spec2]: https://tc39.es/ecma262/#prod-VariableDeclaration
/// [spec3]:  https://tc39.es/ecma262/#sec-declarations-and-the-variable-statement
#[cfg_attr(feature = "serde", derive(serde::Serialize, serde::Deserialize))]
#[cfg_attr(feature = "arbitrary", derive(arbitrary::Arbitrary))]
#[derive(Clone, Debug, PartialEq)]
pub struct Variable {
   binding: Binding,
   init: Option<Expression>,
}

/// Binding represents either an individual binding or a binding pattern.
///
/// More information:
///  - [ECMAScript reference: 14.3 Declarations and the Variable Statement][spec]
///
/// [spec]:  https://tc39.es/ecma262/#sec-declarations-and-the-variable-statement
#[cfg_attr(feature = "serde", derive(serde::Serialize, serde::Deserialize))]
#[cfg_attr(feature = "arbitrary", derive(arbitrary::Arbitrary))]
#[derive(Clone, Debug, PartialEq)]
pub enum Binding {
   /// A single identifier binding.
   Identifier(Identifier),
   /// A pattern binding.
   Pattern(Pattern),
}

/// A **[lexical declaration]** defines variables that are scoped to the lexical environment of
/// the variable declaration.
///
/// [lexical declaration]: https://tc39.es/ecma262/#sec-let-and-const-declarations
#[cfg_attr(feature = "serde", derive(serde::Serialize, serde::Deserialize))]
#[cfg_attr(feature = "arbitrary", derive(arbitrary::Arbitrary))]
#[derive(Clone, Debug, PartialEq)]
pub enum LexicalDeclaration {
   /// A <code>[const]</code> variable creates a constant whose scope can be either global or local
   /// to the block in which it is declared.
   ///
   /// An initializer for a constant is required. You must specify its value in the same statement
   /// in which it's declared. (This makes sense, given that it can't be changed later)
   ///
   /// [const]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/const
   Const(VariableList),

   /// A <code>[let]</code> variable is limited to a scope of a block statement, or expression on
   /// which it is used, unlike the `var` keyword, which defines a variable globally, or locally to
   /// an entire function regardless of block scope.
   ///
   /// Just like const, `let` does not create properties of the window object when declared
   /// globally (in the top-most scope).
   ///
   /// If a let declaration does not have an initializer, the variable is assigned the value `undefined`.
   ///
   /// [let]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/let
   Let(VariableList),
}
```

在 进行赋值语句解析的时候非常复杂，有非常多的可能情况，于是就派出了这么多的分析器：
* ConditionalExpression 条件分析器
* ShortCircuitExpression 逻辑表达式
  * && 
  * || 
  * ??
* FormalParameterList 箭头表达式列表
* 



![引号匹配](引号匹配.png)
![赋值表达式](赋值表达式parser.png)
![块级作用域](块级作用域.png)


## 2.6 分号
在 Binding 创建出来之后，会继续往后面判断是否需要分号，我想这就是 js 引擎的自动插入分号规则的简单版吧

1. BindingList::parser 判断了后面是否由分号，如果没有就插入分号，判断了const 有没有赋值
   调用 cursor.expect_semicolon 来进行是否分号的判断，如果有需要就插入分号
   调用 self.buffered_lexer.next 移动指针进行后面的字符解析
2. 在解析完毕之后，进行排序
   js var 和 function 会进行<span style="color: red">声明提升</span>，应该是干这个的
   **items.sort_by(ast::StatementListItem::hoistable_order);**


![分号规则判断](分号规则判断.png)
![声明排序.png](声明排序.png)


# 3  转换为 ast

```rust
Ok(ast::StatementList::new(items, strict))
```

